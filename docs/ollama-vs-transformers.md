# ollamaå’Œtransformersçš„å¯¹æ¯”
# ğŸ—ï¸ æ¶æ„å®šä½å¯¹æ¯”


ç»´åº¦ |	Ollama |	Transformers
|-------|-------|-------|
å®šä½ |	æœ¬åœ°æ¨¡å‹è¿è¡Œå¹³å°	| æ¨¡å‹æ¨ç†åº“
ä½¿ç”¨æ–¹å¼ |	å‘½ä»¤è¡Œå·¥å…· + APIæœåŠ¡ |	Pythonä»£ç åº“
éƒ¨ç½² |	æœ¬åœ°æœåŠ¡åŒ–éƒ¨ç½² |	ä»£ç çº§é›†æˆ


# ğŸ¯ æ ¸å¿ƒåŒºåˆ«

## Ollama - "å¼€ç®±å³ç”¨"çš„æ¨¡å‹è¿è¡Œå™¨
```bash
# Ollama ä½¿ç”¨æ–¹å¼
ollama run qwen3-coder:480b # è¿„ä»Šä¸ºæ­¢æœ€å…·æœ‰ä»£ç†èƒ½åŠ›çš„ä»£ç æ¨¡å‹
ollama run qwen3-embedding:8b # ç”¨äºæ–‡æœ¬åµŒå…¥å’Œæ’åºä»»åŠ¡
ollama run qwen3-vl:235b # è¿„ä»Šä¸ºæ­¢ Qwen ç³»åˆ—ä¸­åŠŸèƒ½æœ€å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹
```

## Transformers - "æ·±åº¦å®šåˆ¶"çš„å¼€å‘åº“

```python
# Transformers ä½¿ç”¨æ–¹å¼
from transformers import pipeline
import torch

model_id = "openai/gpt-oss-20b"

pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype="auto",
    device_map="auto",
)

messages = [
    {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},
]

outputs = pipe(
    messages,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1])

```

# ğŸ“Š è¯¦ç»†ä¼˜ç¼ºç‚¹åˆ†æ

## Ollama çš„ä¼˜ç‚¹ âœ…

1. æç®€éƒ¨ç½²
```bash
# ä¸€è¡Œå‘½ä»¤å®‰è£…ï¼Œä¸€è¡Œå‘½ä»¤è¿è¡Œ
curl -fsSL https://ollama.ai/install.sh | sh
ollama run llama2
```


2. è‡ªåŠ¨æ¨¡å‹ç®¡ç†
- è‡ªåŠ¨ä¸‹è½½ã€ç‰ˆæœ¬ç®¡ç†
- å†…å­˜ä¼˜åŒ–ã€é‡åŒ–å¤„ç†
- æ— éœ€å…³å¿ƒæ¨¡å‹æ–‡ä»¶ä½ç½®

3. æ ‡å‡†åŒ–API
```python
# ç»Ÿä¸€çš„REST API
import requests
response = requests.post('http://localhost:11434/api/generate', 
                       json={'model': 'llama2', 'prompt': 'Hello'})
```

4. èµ„æºå‹å¥½
- è‡ªåŠ¨CPU/GPUåˆ‡æ¢
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–
- é€‚åˆä¸ªäººç”µè„‘è¿è¡Œ

## Ollama çš„ç¼ºç‚¹ âŒ

1. çµæ´»æ€§æœ‰é™
- æ¨¡å‹å‚æ•°è°ƒæ•´å—é™
- æ— æ³•ä¿®æ”¹æ¨¡å‹æ¶æ„
- æœ‰é™çš„å®šåˆ¶é€‰é¡¹

2. æ¨¡å‹é€‰æ‹©æœ‰é™
- ä¸»è¦æ”¯æŒæµè¡Œå¼€æºæ¨¡å‹
- æ— æ³•è½»æ¾ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹

3. é»‘ç›’æ“ä½œ
- åº•å±‚ç»†èŠ‚è¢«éšè—
- è°ƒè¯•å›°éš¾

## Transformers çš„ä¼˜ç‚¹ âœ…
1. å®Œå…¨æ§åˆ¶
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# ç²¾ç»†æ§åˆ¶æ¯ä¸ªå‚æ•°
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True  # é‡åŒ–é€‰é¡¹
)
```

2. æ¨¡å‹ç”Ÿæ€ä¸°å¯Œ
- æ”¯æŒHugging Faceä¸Šæ‰€æœ‰æ¨¡å‹
- è½»æ¾åˆ‡æ¢ä¸åŒæ¶æ„
- æ”¯æŒè‡ªå®šä¹‰è®­ç»ƒ

3. å¼€å‘çµæ´»æ€§
```python
# å¯ä»¥æ·±åº¦å®šåˆ¶æ¨ç†æµç¨‹
def custom_generation(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
    return tokenizer.decode(outputs[0])
```

## Transformers çš„ç¼ºç‚¹ âŒ
1. é…ç½®å¤æ‚

- éœ€è¦æ‰‹åŠ¨å¤„ç†ç¯å¢ƒé…ç½®

- å†…å­˜ç®¡ç†éœ€è¦ä¸“ä¸šçŸ¥è¯†

- ä¾èµ–é¡¹è¾ƒå¤š

2. éƒ¨ç½²é—¨æ§›é«˜

- éœ€è¦ç¼–å†™æœåŠ¡åŒ–ä»£ç 

- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å¤æ‚

3. èµ„æºè¦æ±‚é«˜

- éœ€è¦æ›´å¤šæŠ€æœ¯çŸ¥è¯†

- è°ƒè¯•å’Œä¼˜åŒ–éœ€è¦ç»éªŒ

# ğŸ¯ é€‚ç”¨åœºæ™¯
### é€‰æ‹© Ollama å½“ï¼š
- ğŸš€ æƒ³è¦å¿«é€Ÿä½“éªŒå¤§æ¨¡å‹

- ğŸ’» åœ¨ä¸ªäººç”µè„‘ä¸Šè¿è¡Œ

- ğŸ”§ ä¸éœ€è¦æ·±åº¦å®šåˆ¶

- ğŸ“± æƒ³è¦ç®€å•çš„APIæ¥å£

- ğŸ¯ ä¸»è¦åšåŸå‹éªŒè¯

### é€‰æ‹© Transformers å½“ï¼š
- ğŸ”¬ éœ€è¦ç ”ç©¶æˆ–å®éªŒ

- ğŸ­ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

- ğŸ›ï¸ éœ€è¦ç²¾ç»†æ§åˆ¶å‚æ•°

- ğŸ”§ è¦ä¿®æ”¹æ¨¡å‹æ¶æ„

- ğŸ“š éœ€è¦å¤šç§æ¨¡å‹ç»„åˆ

# ğŸ’¡ å®é™…ä½¿ç”¨ç¤ºä¾‹

### Ollama å·¥ä½œæµ
```bash
# 1. å®‰è£…
curl -fsSL https://ollama.ai/install.sh | sh

# 2. è¿è¡Œæ¨¡å‹
ollama run qwen3-coder:480b

# 3. APIè°ƒç”¨
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "qwen3-coder:480b",
  "prompt": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"
}'
```

### Transformers å·¥ä½œæµ
```python
# 1. å®‰è£…ç¯å¢ƒ
pip install transformers torch accelerate

# 2. ç¼–å†™ä»£ç 
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "openai/gpt-oss-20b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3. è‡ªå®šä¹‰æ¨ç†é€»è¾‘
def generate_code(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=100)
    return tokenizer.decode(outputs[0])
```

# ğŸª æˆ‘çš„å»ºè®®
## æ–°æ‰‹è·¯çº¿å›¾ï¼š

1. ä» Ollama å¼€å§‹ - å¿«é€Ÿå»ºç«‹ç›´è§‚æ„Ÿå—

2. ç”¨ Transformers æ·±å…¥ - ç†è§£åº•å±‚åŸç†

3. æ ¹æ®éœ€æ±‚é€‰æ‹© - Ollamaç”¨äºå¿«é€Ÿéƒ¨ç½²ï¼ŒTransformersç”¨äºæ·±åº¦å¼€å‘

## ç»„åˆä½¿ç”¨æ–¹æ¡ˆï¼š

1. ç”¨ Ollama å¿«é€ŸéªŒè¯æƒ³æ³•

2. ç”¨ Transformers å®ç°å®šåˆ¶éœ€æ±‚

3. åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥ä¸¤è€…ç»“åˆ